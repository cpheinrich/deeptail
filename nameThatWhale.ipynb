{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started \n",
    "- Download data from: https://www.kaggle.com/c/whale-categorization-playground\n",
    "- Rename train.csv to targets.csv\n",
    "- Rename the train directory to kaggle_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Image', 'Id']\n",
      "9850\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "home_dir = os.getcwd()\n",
    "fname = os.path.join(home_dir, 'targets.csv') # targets for both train and validation\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "lines = lines[:-1]\n",
    "\n",
    "print(header)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the whale ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4251\n",
      "[ 0.  0.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "whale_ids = [line.split(',')[1] for line in lines]\n",
    "whale_ids = set(whale_ids) # convert to set to remove duplicats\n",
    "whale_ids = list(whale_ids) # convert back to list to make it ordered\n",
    "\n",
    "\n",
    "# Am no longer using the whale2vec function, sinc the ImageDataGenerator automatically one-hot-encodes the \n",
    "# targets\n",
    "whale_dict = {}\n",
    "for i, whale in enumerate(whale_ids):\n",
    "    vec = np.zeros(len(whale_ids))\n",
    "    vec[i] = 1\n",
    "    whale_dict[whale] = vec\n",
    "    \n",
    "def whale2vec(whale): # returns a unique one-hot encoded vector given a whale_id\n",
    "    if whale in whale_dict.keys():\n",
    "        return whale_dict[whale]\n",
    "    else:\n",
    "        print(\"whale not found. Returning new_whale vector\")\n",
    "        return whale_dict['new_whale']\n",
    "               \n",
    "vec = whale2vec(whale_ids[2])\n",
    "print(len(whale_ids))\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import errno \n",
    "\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  \n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we split the training images into a train and validation set, and then subdivide them into directories for each whale_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500\n",
      "2350\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "original_dataset_dir = os.path.join(home_dir, 'kaggle_train')\n",
    "\n",
    "train_dir = os.path.join(home_dir, 'train')\n",
    "mkdir_p(train_dir)\n",
    "\n",
    "validation_dir = os.path.join(home_dir, 'validation')\n",
    "mkdir_p(validation_dir)\n",
    "\n",
    "# Let's then create a subdirectory for each whale_id in both the train and validation directories \n",
    "# so we can use the ImageDataGenerator magic function\n",
    "\n",
    "classes_count = len(whale_ids) # During the development phase of testing models, we dont look at all 4251 classes. \n",
    "                     # When we're ready to look at all classes we can set classes_count = len(whale_ids)\n",
    "\n",
    "for i, whale_id in enumerate(whale_ids):\n",
    "    if i < classes_count:\n",
    "        mkdir_p(os.path.join(train_dir, whale_id))\n",
    "        mkdir_p(os.path.join(validation_dir, whale_id))\n",
    "\n",
    "train_image_count = 0\n",
    "\n",
    "# Copy first 7500 files into the appropriate whale directory in train dir (only if their class is included)   \n",
    "for i in range(7500):\n",
    "    pic = lines[i].split(',')[0]\n",
    "    whale_id = lines[i].split(',')[1]\n",
    "    src = os.path.join(original_dataset_dir, pic)\n",
    "    whale_id_dir = os.path.join(train_dir, whale_id)\n",
    "    if os.path.isdir(whale_id_dir):\n",
    "        dst = os.path.join(whale_id_dir, pic)\n",
    "        shutil.copyfile(src, dst)\n",
    "        train_image_count += 1\n",
    "\n",
    "validation_image_count = 0\n",
    "# copy the rest into the appropriate whale directory in validation dir    (only if their class is included)  \n",
    "for i in range(7500,len(lines)):\n",
    "    pic = lines[i].split(',')[0]\n",
    "    whale_id = lines[i].split(',')[1]\n",
    "    src = os.path.join(original_dataset_dir, pic)\n",
    "    whale_id_dir = os.path.join(validation_dir, whale_id)\n",
    "    if os.path.isdir(whale_id_dir):\n",
    "        dst = os.path.join(whale_id_dir, pic)\n",
    "        shutil.copyfile(src, dst)\n",
    "        validation_image_count += 1\n",
    "\n",
    "print(train_image_count)\n",
    "print(validation_image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 336 images belonging to 200 classes.\n",
      "Found 95 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "image_size = (180,180) #adjustable parameter for processed image_size. Run time should \n",
    "batch_size = 10\n",
    "color_mode='rgb'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to image_size - define above\n",
    "        target_size=image_size,\n",
    "        color_mode=color_mode, # input images are RGB and grayscale, but we map them all onto \n",
    "                                # which should be suitable for whale tails anyway\n",
    "        batch_size=batch_size,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        validation_dir,\n",
    "        # All images will be resized to image_size - define above\n",
    "        target_size=image_size,\n",
    "        color_mode=color_mode,\n",
    "        batch_size=batch_size,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(image_size[0],image_size[1],3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(2*classes_count, activation='relu')) # I use 2*classes_count as a first guess\n",
    "                                                            # should be tested to see what works best\n",
    "model.add(layers.Dense(classes_count, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 178, 178, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 89, 89, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 87, 87, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 41, 41, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 18, 18, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               4147600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               80200     \n",
      "=================================================================\n",
      "Total params: 4,468,632\n",
      "Trainable params: 4,468,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "34/34 [==============================] - 22s 659ms/step - loss: 5.2869 - acc: 0.0353 - val_loss: 5.1927 - val_acc: 0.0526\n",
      "Epoch 2/5\n",
      "34/34 [==============================] - 23s 676ms/step - loss: 5.1587 - acc: 0.0471 - val_loss: 5.1444 - val_acc: 0.0526\n",
      "Epoch 3/5\n",
      "34/34 [==============================] - 20s 593ms/step - loss: 5.0622 - acc: 0.0471 - val_loss: 5.1843 - val_acc: 0.0526\n",
      "Epoch 4/5\n",
      "34/34 [==============================] - 21s 621ms/step - loss: 4.9443 - acc: 0.0656 - val_loss: 5.2003 - val_acc: 0.0526\n",
      "Epoch 5/5\n",
      "34/34 [==============================] - 20s 601ms/step - loss: 4.8181 - acc: 0.0706 - val_loss: 5.2453 - val_acc: 0.0526\n"
     ]
    }
   ],
   "source": [
    "test_steps = int(round(train_image_count/batch_size))\n",
    "validation_steps = int(round(validation_image_count/batch_size))\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=test_steps, # batch size is 20, 375 steps will get us through 7500 images\n",
    "      epochs=5,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using a pre-trained convolutional base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications import Xception\n",
    "\n",
    "conv_base = Xception(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(image_size[0], image_size[1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 180, 180, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 89, 89, 32)   864         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 89, 89, 32)   128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 89, 89, 32)   0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 87, 87, 64)   18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 87, 87, 64)   256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 87, 87, 64)   0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 87, 87, 128)  8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 87, 87, 128)  512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 87, 87, 128)  0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 87, 87, 128)  17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 87, 87, 128)  512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 44, 44, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 44, 44, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 44, 44, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 44, 44, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 44, 44, 128)  0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 44, 44, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 44, 44, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 44, 44, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 44, 44, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 44, 44, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 22, 22, 256)  32768       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 22, 22, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 22, 22, 256)  1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 22, 22, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 22, 22, 256)  0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 22, 22, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 22, 22, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 22, 22, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 22, 22, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 22, 22, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 11, 11, 728)  186368      add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 11, 11, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 11, 11, 728)  2912        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 11, 11, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 11, 11, 728)  0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 11, 11, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 11, 11, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 11, 11, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 11, 11, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 11, 11, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 11, 11, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 11, 11, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 11, 11, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 11, 11, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 11, 11, 728)  0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 11, 11, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 11, 11, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 11, 11, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 11, 11, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 11, 11, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 11, 11, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 11, 11, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 11, 11, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 11, 11, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 11, 11, 728)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 11, 11, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 11, 11, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 11, 11, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 11, 11, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 11, 11, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 11, 11, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 11, 11, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 11, 11, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 11, 11, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 11, 11, 728)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 11, 11, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 11, 11, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 11, 11, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 11, 11, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 11, 11, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 11, 11, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 11, 11, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 11, 11, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 11, 11, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 11, 11, 728)  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 11, 11, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 11, 11, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 11, 11, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 11, 11, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 11, 11, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 11, 11, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 11, 11, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 11, 11, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 11, 11, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 11, 11, 728)  0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 11, 11, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 11, 11, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 11, 11, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 11, 11, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 11, 11, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 11, 11, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 11, 11, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 11, 11, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 11, 11, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 11, 11, 728)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 11, 11, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 11, 11, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 11, 11, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 11, 11, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 11, 11, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 11, 11, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 11, 11, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 11, 11, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 11, 11, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 11, 11, 728)  0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 11, 11, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 11, 11, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 11, 11, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 11, 11, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 11, 11, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 11, 11, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 11, 11, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 11, 11, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 11, 11, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 11, 11, 728)  0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 11, 11, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 11, 11, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 11, 11, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 11, 11, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 11, 11, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 6, 6, 1024)   745472      add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 6, 6, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 6, 6, 1024)   4096        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 6, 6, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 6, 6, 1536)   1582080     add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 6, 6, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 6, 6, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 6, 6, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 6, 6, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 6, 6, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 20,861,480\n",
      "Trainable params: 20,806,952\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final feature map has shape `(6, 6, 2048)`. That's the feature on top of which we will stick a densely-connected classifier.\n",
    "\n",
    "At this point, there are two ways we could proceed: \n",
    "\n",
    "* Running the convolutional base over our dataset, recording its output to a Numpy array on disk, then using this data as input to a \n",
    "standalone densely-connected classifier similar to those you have seen in the first chapters of this book. This solution is very fast and \n",
    "cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the \n",
    "most expensive part of the pipeline. However, for the exact same reason, this technique would not allow us to leverage data augmentation at \n",
    "all.\n",
    "* Extending the model we have (`conv_base`) by adding `Dense` layers on top, and running the whole thing end-to-end on the input data. This \n",
    "allows us to use data augmentation, because every input image is going through the convolutional base every time it is seen by the model. \n",
    "However, for this same reason, this technique is far more expensive than the first one.\n",
    "\n",
    "We will cover both techniques. Let's walk through the code required to set-up the first one: recording the output of `conv_base` on our \n",
    "data and using these outputs as inputs to a new model.\n",
    "\n",
    "We will start by simply running instances of the previously-introduced `ImageDataGenerator` to extract images as Numpy arrays as well as \n",
    "their labels. We will extract features from these images simply by calling the `predict` method of the `conv_base` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 331 images belonging to 200 classes.\n",
      "Found 85 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "home_dir = os.getcwd()\n",
    "train_dir = os.path.join(home_dir, 'train')\n",
    "validation_dir = os.path.join(home_dir, 'validation')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 6, 6, 2048))\n",
    "    labels = np.zeros(shape=(sample_count, classes_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=image_size,\n",
    "        color_mode=color_mode,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(train_dir, train_image_count)\n",
    "validation_features, validation_labels = extract_features(validation_dir, validation_image_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features,(train_image_count, 6*6*2048))\n",
    "validation_features = np.reshape(validation_features, (validation_image_count, 6*6*2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 128)               9437312   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               25800     \n",
      "=================================================================\n",
      "Total params: 9,463,112\n",
      "Trainable params: 9,463,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_dim=6*6*2048)) # we can play around with \n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(classes_count, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 331 samples, validate on 85 samples\n",
      "Epoch 1/200\n",
      "331/331 [==============================] - 3s 8ms/step - loss: 4.1132 - acc: 0.1692 - val_loss: 4.9069 - val_acc: 0.1294\n",
      "Epoch 2/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 3.7330 - acc: 0.2538 - val_loss: 4.8456 - val_acc: 0.1176\n",
      "Epoch 3/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 3.4525 - acc: 0.3353 - val_loss: 4.8202 - val_acc: 0.1412\n",
      "Epoch 4/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 3.2310 - acc: 0.3807 - val_loss: 4.7795 - val_acc: 0.1529\n",
      "Epoch 5/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 3.0827 - acc: 0.4169 - val_loss: 4.7338 - val_acc: 0.1647\n",
      "Epoch 6/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 2.8373 - acc: 0.4773 - val_loss: 4.6891 - val_acc: 0.1529\n",
      "Epoch 7/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 2.6402 - acc: 0.4924 - val_loss: 4.6652 - val_acc: 0.1647\n",
      "Epoch 8/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 2.4593 - acc: 0.5257 - val_loss: 4.6789 - val_acc: 0.1765\n",
      "Epoch 9/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 2.3047 - acc: 0.5801 - val_loss: 4.6018 - val_acc: 0.1765\n",
      "Epoch 10/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 2.1396 - acc: 0.6435 - val_loss: 4.5854 - val_acc: 0.2000\n",
      "Epoch 11/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 2.0533 - acc: 0.6586 - val_loss: 4.5704 - val_acc: 0.1882\n",
      "Epoch 12/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 1.9247 - acc: 0.6767 - val_loss: 4.5790 - val_acc: 0.1647\n",
      "Epoch 13/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 1.7373 - acc: 0.6888 - val_loss: 4.6052 - val_acc: 0.2118\n",
      "Epoch 14/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 1.6483 - acc: 0.7311 - val_loss: 4.4882 - val_acc: 0.2000\n",
      "Epoch 15/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 1.5207 - acc: 0.7553 - val_loss: 4.5274 - val_acc: 0.1882\n",
      "Epoch 16/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 1.3877 - acc: 0.8127 - val_loss: 4.5019 - val_acc: 0.2353\n",
      "Epoch 17/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 1.2724 - acc: 0.8097 - val_loss: 4.4670 - val_acc: 0.2235\n",
      "Epoch 18/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 1.2429 - acc: 0.8187 - val_loss: 4.4821 - val_acc: 0.2235\n",
      "Epoch 19/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 1.0937 - acc: 0.8459 - val_loss: 4.5248 - val_acc: 0.2235\n",
      "Epoch 20/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 1.0711 - acc: 0.8369 - val_loss: 4.4718 - val_acc: 0.2353\n",
      "Epoch 21/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.9626 - acc: 0.8761 - val_loss: 4.4984 - val_acc: 0.2353\n",
      "Epoch 22/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.9546 - acc: 0.8912 - val_loss: 4.4902 - val_acc: 0.2471\n",
      "Epoch 23/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.8426 - acc: 0.9063 - val_loss: 4.5214 - val_acc: 0.2471\n",
      "Epoch 24/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.8150 - acc: 0.8882 - val_loss: 4.5187 - val_acc: 0.2353\n",
      "Epoch 25/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.7608 - acc: 0.9245 - val_loss: 4.5059 - val_acc: 0.2588\n",
      "Epoch 26/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.7559 - acc: 0.9154 - val_loss: 4.4892 - val_acc: 0.2471\n",
      "Epoch 27/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.6798 - acc: 0.9154 - val_loss: 4.5573 - val_acc: 0.2588\n",
      "Epoch 28/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.6217 - acc: 0.9456 - val_loss: 4.5351 - val_acc: 0.2471\n",
      "Epoch 29/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.5685 - acc: 0.9396 - val_loss: 4.5028 - val_acc: 0.2353\n",
      "Epoch 30/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.5240 - acc: 0.9366 - val_loss: 4.4846 - val_acc: 0.2353\n",
      "Epoch 31/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.5299 - acc: 0.9305 - val_loss: 4.5441 - val_acc: 0.2471\n",
      "Epoch 32/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.4292 - acc: 0.9698 - val_loss: 4.6080 - val_acc: 0.2471\n",
      "Epoch 33/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.4699 - acc: 0.9577 - val_loss: 4.5249 - val_acc: 0.2588\n",
      "Epoch 34/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.4058 - acc: 0.9698 - val_loss: 4.5717 - val_acc: 0.2706\n",
      "Epoch 35/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.4537 - acc: 0.9517 - val_loss: 4.5409 - val_acc: 0.2588\n",
      "Epoch 36/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.3755 - acc: 0.9668 - val_loss: 4.5840 - val_acc: 0.2706\n",
      "Epoch 37/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.3695 - acc: 0.9547 - val_loss: 4.6185 - val_acc: 0.2588\n",
      "Epoch 38/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.3498 - acc: 0.9758 - val_loss: 4.5630 - val_acc: 0.2588\n",
      "Epoch 39/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.3237 - acc: 0.9728 - val_loss: 4.5934 - val_acc: 0.2471\n",
      "Epoch 40/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.2852 - acc: 0.9849 - val_loss: 4.5935 - val_acc: 0.2588\n",
      "Epoch 41/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.2771 - acc: 0.9819 - val_loss: 4.6440 - val_acc: 0.2471\n",
      "Epoch 42/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.2622 - acc: 0.9849 - val_loss: 4.6482 - val_acc: 0.2588\n",
      "Epoch 43/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.2409 - acc: 0.9758 - val_loss: 4.6229 - val_acc: 0.2824\n",
      "Epoch 44/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.2643 - acc: 0.9849 - val_loss: 4.5854 - val_acc: 0.2824\n",
      "Epoch 45/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1888 - acc: 0.9940 - val_loss: 4.6797 - val_acc: 0.2706\n",
      "Epoch 46/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1875 - acc: 0.9758 - val_loss: 4.6830 - val_acc: 0.2824\n",
      "Epoch 47/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.2155 - acc: 0.9970 - val_loss: 4.6232 - val_acc: 0.2824\n",
      "Epoch 48/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1911 - acc: 0.9879 - val_loss: 4.6898 - val_acc: 0.2706\n",
      "Epoch 49/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1892 - acc: 0.9940 - val_loss: 4.6994 - val_acc: 0.2706\n",
      "Epoch 50/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.2138 - acc: 0.9758 - val_loss: 4.6977 - val_acc: 0.2706\n",
      "Epoch 51/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1741 - acc: 0.9819 - val_loss: 4.6611 - val_acc: 0.2824\n",
      "Epoch 52/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1525 - acc: 0.9879 - val_loss: 4.6953 - val_acc: 0.2706\n",
      "Epoch 53/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1514 - acc: 0.9849 - val_loss: 4.7156 - val_acc: 0.2706\n",
      "Epoch 54/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1746 - acc: 0.9879 - val_loss: 4.7000 - val_acc: 0.2824\n",
      "Epoch 55/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1499 - acc: 0.9909 - val_loss: 4.7126 - val_acc: 0.2706\n",
      "Epoch 56/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1376 - acc: 0.9909 - val_loss: 4.6918 - val_acc: 0.2824\n",
      "Epoch 57/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1542 - acc: 0.9849 - val_loss: 4.7444 - val_acc: 0.2706\n",
      "Epoch 58/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1450 - acc: 0.9970 - val_loss: 4.7138 - val_acc: 0.2706\n",
      "Epoch 59/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1191 - acc: 0.9879 - val_loss: 4.7903 - val_acc: 0.2588\n",
      "Epoch 60/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1164 - acc: 0.9970 - val_loss: 4.7837 - val_acc: 0.2706\n",
      "Epoch 61/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1133 - acc: 1.0000 - val_loss: 4.7723 - val_acc: 0.2706\n",
      "Epoch 62/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1250 - acc: 0.9849 - val_loss: 4.8915 - val_acc: 0.2706\n",
      "Epoch 63/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1135 - acc: 0.9940 - val_loss: 4.7780 - val_acc: 0.2706\n",
      "Epoch 64/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.1071 - acc: 0.9879 - val_loss: 4.8152 - val_acc: 0.2941\n",
      "Epoch 65/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0979 - acc: 1.0000 - val_loss: 4.8194 - val_acc: 0.2706\n",
      "Epoch 66/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0812 - acc: 1.0000 - val_loss: 4.8313 - val_acc: 0.2706\n",
      "Epoch 67/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0936 - acc: 1.0000 - val_loss: 4.8597 - val_acc: 0.2706\n",
      "Epoch 68/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0980 - acc: 0.9879 - val_loss: 4.8606 - val_acc: 0.2706\n",
      "Epoch 69/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0763 - acc: 0.9970 - val_loss: 4.8832 - val_acc: 0.2706\n",
      "Epoch 70/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0942 - acc: 0.9879 - val_loss: 4.8820 - val_acc: 0.2588\n",
      "Epoch 71/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0858 - acc: 0.9909 - val_loss: 4.8689 - val_acc: 0.2588\n",
      "Epoch 72/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0901 - acc: 0.9849 - val_loss: 4.8163 - val_acc: 0.2706\n",
      "Epoch 73/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0811 - acc: 1.0000 - val_loss: 4.8840 - val_acc: 0.2706\n",
      "Epoch 74/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0873 - acc: 0.9909 - val_loss: 4.8158 - val_acc: 0.2706\n",
      "Epoch 75/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0737 - acc: 0.9970 - val_loss: 4.9179 - val_acc: 0.2706\n",
      "Epoch 76/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0780 - acc: 0.9879 - val_loss: 4.8360 - val_acc: 0.2588\n",
      "Epoch 77/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0680 - acc: 0.9970 - val_loss: 4.9171 - val_acc: 0.2588\n",
      "Epoch 78/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0646 - acc: 0.9970 - val_loss: 4.8317 - val_acc: 0.2706\n",
      "Epoch 79/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0499 - acc: 0.9970 - val_loss: 4.9617 - val_acc: 0.2706\n",
      "Epoch 80/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0509 - acc: 1.0000 - val_loss: 4.9499 - val_acc: 0.2706\n",
      "Epoch 81/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0523 - acc: 1.0000 - val_loss: 4.9558 - val_acc: 0.2824\n",
      "Epoch 82/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0683 - acc: 0.9909 - val_loss: 4.9592 - val_acc: 0.2824\n",
      "Epoch 83/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0690 - acc: 0.9909 - val_loss: 4.8796 - val_acc: 0.2588\n",
      "Epoch 84/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0545 - acc: 1.0000 - val_loss: 4.9885 - val_acc: 0.2824\n",
      "Epoch 85/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0640 - acc: 0.9909 - val_loss: 4.9636 - val_acc: 0.2824\n",
      "Epoch 86/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0503 - acc: 0.9940 - val_loss: 4.9575 - val_acc: 0.2706\n",
      "Epoch 87/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0526 - acc: 0.9970 - val_loss: 4.9708 - val_acc: 0.2824\n",
      "Epoch 88/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0628 - acc: 0.9909 - val_loss: 5.0170 - val_acc: 0.2706\n",
      "Epoch 89/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0420 - acc: 1.0000 - val_loss: 5.0334 - val_acc: 0.2588\n",
      "Epoch 90/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0618 - acc: 0.9909 - val_loss: 4.9783 - val_acc: 0.2471\n",
      "Epoch 91/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0530 - acc: 0.9970 - val_loss: 5.0372 - val_acc: 0.2471\n",
      "Epoch 92/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0401 - acc: 1.0000 - val_loss: 5.0660 - val_acc: 0.2588\n",
      "Epoch 93/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0318 - acc: 1.0000 - val_loss: 5.1115 - val_acc: 0.2588\n",
      "Epoch 94/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0413 - acc: 0.9970 - val_loss: 5.0234 - val_acc: 0.2706\n",
      "Epoch 95/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0540 - acc: 1.0000 - val_loss: 5.0538 - val_acc: 0.2471\n",
      "Epoch 96/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0401 - acc: 1.0000 - val_loss: 5.0956 - val_acc: 0.2706\n",
      "Epoch 97/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0534 - acc: 0.9970 - val_loss: 5.1577 - val_acc: 0.2588\n",
      "Epoch 98/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0376 - acc: 0.9970 - val_loss: 5.1429 - val_acc: 0.2706\n",
      "Epoch 99/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0423 - acc: 0.9970 - val_loss: 5.1223 - val_acc: 0.2588\n",
      "Epoch 100/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0285 - acc: 1.0000 - val_loss: 5.1521 - val_acc: 0.2706\n",
      "Epoch 101/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0341 - acc: 0.9970 - val_loss: 5.1523 - val_acc: 0.2706\n",
      "Epoch 102/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0327 - acc: 1.0000 - val_loss: 5.1430 - val_acc: 0.2706\n",
      "Epoch 103/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0342 - acc: 0.9970 - val_loss: 5.1417 - val_acc: 0.2824\n",
      "Epoch 104/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0344 - acc: 1.0000 - val_loss: 5.1257 - val_acc: 0.2706\n",
      "Epoch 105/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0507 - acc: 0.9909 - val_loss: 5.0868 - val_acc: 0.2706\n",
      "Epoch 106/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0348 - acc: 0.9970 - val_loss: 5.1987 - val_acc: 0.2824\n",
      "Epoch 107/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0358 - acc: 0.9970 - val_loss: 5.0876 - val_acc: 0.2588\n",
      "Epoch 108/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0308 - acc: 0.9940 - val_loss: 5.1634 - val_acc: 0.2588\n",
      "Epoch 109/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0257 - acc: 0.9970 - val_loss: 5.1868 - val_acc: 0.2588\n",
      "Epoch 110/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0308 - acc: 1.0000 - val_loss: 5.1122 - val_acc: 0.2588\n",
      "Epoch 111/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0523 - acc: 0.9940 - val_loss: 5.1920 - val_acc: 0.2824\n",
      "Epoch 112/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0213 - acc: 1.0000 - val_loss: 5.1888 - val_acc: 0.2706\n",
      "Epoch 113/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0273 - acc: 0.9970 - val_loss: 5.2873 - val_acc: 0.2588\n",
      "Epoch 114/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0331 - acc: 0.9940 - val_loss: 5.3438 - val_acc: 0.2706\n",
      "Epoch 115/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0262 - acc: 1.0000 - val_loss: 5.2762 - val_acc: 0.2706\n",
      "Epoch 116/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0209 - acc: 1.0000 - val_loss: 5.2594 - val_acc: 0.2706\n",
      "Epoch 117/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0350 - acc: 0.9970 - val_loss: 5.2430 - val_acc: 0.2588\n",
      "Epoch 118/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0257 - acc: 1.0000 - val_loss: 5.2376 - val_acc: 0.2706\n",
      "Epoch 119/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0217 - acc: 1.0000 - val_loss: 5.2936 - val_acc: 0.2824\n",
      "Epoch 120/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0290 - acc: 0.9970 - val_loss: 5.3100 - val_acc: 0.2824\n",
      "Epoch 121/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0196 - acc: 1.0000 - val_loss: 5.2711 - val_acc: 0.2706\n",
      "Epoch 122/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0181 - acc: 1.0000 - val_loss: 5.4145 - val_acc: 0.2706\n",
      "Epoch 123/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0231 - acc: 1.0000 - val_loss: 5.3218 - val_acc: 0.2824\n",
      "Epoch 124/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0292 - acc: 0.9970 - val_loss: 5.3392 - val_acc: 0.3059\n",
      "Epoch 125/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0225 - acc: 1.0000 - val_loss: 5.3518 - val_acc: 0.2824\n",
      "Epoch 126/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0333 - acc: 0.9940 - val_loss: 5.2641 - val_acc: 0.2824\n",
      "Epoch 127/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0211 - acc: 1.0000 - val_loss: 5.3186 - val_acc: 0.2706\n",
      "Epoch 128/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0208 - acc: 1.0000 - val_loss: 5.2950 - val_acc: 0.2706\n",
      "Epoch 129/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0263 - acc: 1.0000 - val_loss: 5.4195 - val_acc: 0.2588\n",
      "Epoch 130/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0185 - acc: 1.0000 - val_loss: 5.3633 - val_acc: 0.2824\n",
      "Epoch 131/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0184 - acc: 1.0000 - val_loss: 5.3871 - val_acc: 0.2824\n",
      "Epoch 132/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0331 - acc: 0.9909 - val_loss: 5.3273 - val_acc: 0.2824\n",
      "Epoch 133/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0292 - acc: 0.9970 - val_loss: 5.3647 - val_acc: 0.2706\n",
      "Epoch 134/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0171 - acc: 1.0000 - val_loss: 5.2864 - val_acc: 0.2941\n",
      "Epoch 135/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0172 - acc: 1.0000 - val_loss: 5.3345 - val_acc: 0.2588\n",
      "Epoch 136/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0256 - acc: 0.9970 - val_loss: 5.3938 - val_acc: 0.2588\n",
      "Epoch 137/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0189 - acc: 1.0000 - val_loss: 5.4475 - val_acc: 0.2588\n",
      "Epoch 138/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0264 - acc: 0.9970 - val_loss: 5.3736 - val_acc: 0.2706\n",
      "Epoch 139/200\n",
      "331/331 [==============================] - 2s 5ms/step - loss: 0.0130 - acc: 1.0000 - val_loss: 5.3282 - val_acc: 0.2588\n",
      "Epoch 140/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0142 - acc: 1.0000 - val_loss: 5.4222 - val_acc: 0.2706\n",
      "Epoch 141/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0164 - acc: 1.0000 - val_loss: 5.4402 - val_acc: 0.2706\n",
      "Epoch 142/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0212 - acc: 1.0000 - val_loss: 5.4207 - val_acc: 0.2706\n",
      "Epoch 143/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0304 - acc: 0.9940 - val_loss: 5.4064 - val_acc: 0.2588\n",
      "Epoch 144/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0209 - acc: 1.0000 - val_loss: 5.4326 - val_acc: 0.2588\n",
      "Epoch 145/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0151 - acc: 1.0000 - val_loss: 5.4048 - val_acc: 0.2588\n",
      "Epoch 146/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0194 - acc: 0.9970 - val_loss: 5.3770 - val_acc: 0.2824\n",
      "Epoch 147/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0131 - acc: 0.9970 - val_loss: 5.5156 - val_acc: 0.2588\n",
      "Epoch 148/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0155 - acc: 1.0000 - val_loss: 5.4902 - val_acc: 0.2941\n",
      "Epoch 149/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0086 - acc: 1.0000 - val_loss: 5.5544 - val_acc: 0.2941\n",
      "Epoch 150/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0137 - acc: 1.0000 - val_loss: 5.5367 - val_acc: 0.2588\n",
      "Epoch 151/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0121 - acc: 1.0000 - val_loss: 5.6526 - val_acc: 0.2588\n",
      "Epoch 152/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0065 - acc: 1.0000 - val_loss: 5.5584 - val_acc: 0.2824\n",
      "Epoch 153/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0130 - acc: 1.0000 - val_loss: 5.4615 - val_acc: 0.2824\n",
      "Epoch 154/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0173 - acc: 0.9970 - val_loss: 5.5270 - val_acc: 0.2706\n",
      "Epoch 155/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0117 - acc: 1.0000 - val_loss: 5.5195 - val_acc: 0.2941\n",
      "Epoch 156/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0215 - acc: 0.9970 - val_loss: 5.5637 - val_acc: 0.2824\n",
      "Epoch 157/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0134 - acc: 1.0000 - val_loss: 5.5736 - val_acc: 0.2824\n",
      "Epoch 158/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0159 - acc: 1.0000 - val_loss: 5.6357 - val_acc: 0.2941\n",
      "Epoch 159/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0152 - acc: 1.0000 - val_loss: 5.5359 - val_acc: 0.2941\n",
      "Epoch 160/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 5.5948 - val_acc: 0.2941\n",
      "Epoch 161/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 5.6464 - val_acc: 0.3059\n",
      "Epoch 162/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0078 - acc: 1.0000 - val_loss: 5.6701 - val_acc: 0.3059\n",
      "Epoch 163/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0116 - acc: 1.0000 - val_loss: 5.5931 - val_acc: 0.2941\n",
      "Epoch 164/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0163 - acc: 1.0000 - val_loss: 5.6269 - val_acc: 0.2588\n",
      "Epoch 165/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0122 - acc: 1.0000 - val_loss: 5.6161 - val_acc: 0.2706\n",
      "Epoch 166/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0114 - acc: 1.0000 - val_loss: 5.6081 - val_acc: 0.2824\n",
      "Epoch 167/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0195 - acc: 0.9970 - val_loss: 5.6175 - val_acc: 0.3059\n",
      "Epoch 168/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0293 - acc: 0.9970 - val_loss: 5.6017 - val_acc: 0.2706\n",
      "Epoch 169/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 5.6349 - val_acc: 0.2824\n",
      "Epoch 170/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0117 - acc: 1.0000 - val_loss: 5.6215 - val_acc: 0.2824\n",
      "Epoch 171/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0119 - acc: 1.0000 - val_loss: 5.6154 - val_acc: 0.2941\n",
      "Epoch 172/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0153 - acc: 0.9970 - val_loss: 5.6458 - val_acc: 0.2941\n",
      "Epoch 173/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 5.6931 - val_acc: 0.2824\n",
      "Epoch 174/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0190 - acc: 0.9970 - val_loss: 5.6341 - val_acc: 0.3059\n",
      "Epoch 175/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0108 - acc: 1.0000 - val_loss: 5.6388 - val_acc: 0.2941\n",
      "Epoch 176/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0093 - acc: 1.0000 - val_loss: 5.6186 - val_acc: 0.2706\n",
      "Epoch 177/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0170 - acc: 0.9970 - val_loss: 5.6798 - val_acc: 0.2941\n",
      "Epoch 178/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 5.7306 - val_acc: 0.3059\n",
      "Epoch 179/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0083 - acc: 1.0000 - val_loss: 5.7185 - val_acc: 0.2706\n",
      "Epoch 180/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0061 - acc: 1.0000 - val_loss: 5.7176 - val_acc: 0.2706\n",
      "Epoch 181/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0102 - acc: 1.0000 - val_loss: 5.6757 - val_acc: 0.2588\n",
      "Epoch 182/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 5.7315 - val_acc: 0.2706\n",
      "Epoch 183/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0158 - acc: 0.9970 - val_loss: 5.6639 - val_acc: 0.2824\n",
      "Epoch 184/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0113 - acc: 1.0000 - val_loss: 5.6081 - val_acc: 0.3059\n",
      "Epoch 185/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0086 - acc: 1.0000 - val_loss: 5.7247 - val_acc: 0.2941\n",
      "Epoch 186/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0113 - acc: 0.9970 - val_loss: 5.7738 - val_acc: 0.2471\n",
      "Epoch 187/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 5.6998 - val_acc: 0.2824\n",
      "Epoch 188/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0135 - acc: 0.9970 - val_loss: 5.7416 - val_acc: 0.2941\n",
      "Epoch 189/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0078 - acc: 1.0000 - val_loss: 5.7860 - val_acc: 0.3059\n",
      "Epoch 190/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0131 - acc: 0.9970 - val_loss: 5.7387 - val_acc: 0.2941\n",
      "Epoch 191/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0091 - acc: 1.0000 - val_loss: 5.7808 - val_acc: 0.2941\n",
      "Epoch 192/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 5.7227 - val_acc: 0.2941\n",
      "Epoch 193/200\n",
      "331/331 [==============================] - 2s 6ms/step - loss: 0.0178 - acc: 0.9970 - val_loss: 5.7295 - val_acc: 0.2941\n",
      "Epoch 194/200\n",
      "331/331 [==============================] - 2s 7ms/step - loss: 0.0163 - acc: 0.9970 - val_loss: 5.8014 - val_acc: 0.2824\n",
      "Epoch 195/200\n",
      "331/331 [==============================] - 2s 7ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 5.7903 - val_acc: 0.2941\n",
      "Epoch 196/200\n",
      "331/331 [==============================] - 2s 7ms/step - loss: 0.0203 - acc: 0.9970 - val_loss: 5.8191 - val_acc: 0.3059\n",
      "Epoch 197/200\n",
      "331/331 [==============================] - 2s 7ms/step - loss: 0.0131 - acc: 1.0000 - val_loss: 5.8658 - val_acc: 0.2588\n",
      "Epoch 198/200\n",
      "331/331 [==============================] - 2s 7ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 5.7744 - val_acc: 0.3059\n",
      "Epoch 199/200\n",
      "331/331 [==============================] - 2s 7ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 5.7857 - val_acc: 0.2941\n",
      "Epoch 200/200\n",
      "331/331 [==============================] - 2s 7ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 5.8139 - val_acc: 0.2824\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=200,\n",
    "                    batch_size=20,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('name_that_whale_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 512)               37749248  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4251)              2180763   \n",
      "=================================================================\n",
      "Total params: 39,930,011\n",
      "Trainable params: 39,930,011\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('name_that_whale_1.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(classes_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xception (Model)             (None, 6, 6, 2048)        20861480  \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 73728)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               18874624  \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 200)               51400     \n",
      "=================================================================\n",
      "Total params: 39,787,504\n",
      "Trainable params: 18,926,024\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we compile and train our model, a very important thing to do is to freeze the convolutional base. \"Freezing\" a layer or set of layers means preventing their weights from getting updated during training. If we don't do this, then the representations that were previously learned by the convolutional base would get modified during training. Since the Dense layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned.\n",
    "\n",
    "In Keras, freezing a network is done by setting its trainable attribute to False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this setup, only the weights from the two Dense layers that we added will be trained. That's a total of four weight tensors: two per layer (the main weight matrix and the bias vector). Note that in order for these changes to take effect, we must first compile the model. If you ever modify weight trainability after compilation, you should then re-compile the model, or these changes would be ignored.\n",
    "\n",
    "Now we can start training our model, with the same data augmentation configuration that we used in our previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 331 images belonging to 200 classes.\n",
      "Found 85 images belonging to 200 classes.\n",
      "test steps: 33\n",
      "validation_steps 8\n",
      "Epoch 1/30\n",
      "33/33 [==============================] - 83s 3s/step - loss: 2.9263 - acc: 0.3852 - val_loss: 4.8238 - val_acc: 0.2000\n",
      "Epoch 2/30\n",
      "33/33 [==============================] - 81s 2s/step - loss: 2.6687 - acc: 0.4610 - val_loss: 4.8453 - val_acc: 0.2000\n",
      "Epoch 3/30\n",
      "33/33 [==============================] - 80s 2s/step - loss: 2.4453 - acc: 0.5186 - val_loss: 4.8819 - val_acc: 0.2250\n",
      "Epoch 4/30\n",
      "33/33 [==============================] - 82s 2s/step - loss: 2.2864 - acc: 0.5606 - val_loss: 4.8428 - val_acc: 0.2375\n",
      "Epoch 5/30\n",
      "33/33 [==============================] - 78s 2s/step - loss: 2.2520 - acc: 0.5889 - val_loss: 4.8069 - val_acc: 0.2250\n",
      "Epoch 6/30\n",
      "33/33 [==============================] - 83s 3s/step - loss: 2.1198 - acc: 0.5580 - val_loss: 4.8422 - val_acc: 0.2000\n",
      "Epoch 7/30\n",
      "33/33 [==============================] - 80s 2s/step - loss: 1.8977 - acc: 0.6187 - val_loss: 4.9516 - val_acc: 0.2250\n",
      "Epoch 8/30\n",
      "33/33 [==============================] - 80s 2s/step - loss: 2.0031 - acc: 0.6126 - val_loss: 4.9256 - val_acc: 0.2250\n",
      "Epoch 9/30\n",
      "33/33 [==============================] - 79s 2s/step - loss: 1.7375 - acc: 0.6339 - val_loss: 5.0290 - val_acc: 0.2500\n",
      "Epoch 10/30\n",
      "33/33 [==============================] - 80s 2s/step - loss: 1.6669 - acc: 0.6490 - val_loss: 5.0568 - val_acc: 0.2500\n",
      "Epoch 11/30\n",
      "33/33 [==============================] - 79s 2s/step - loss: 1.5989 - acc: 0.7036 - val_loss: 5.0311 - val_acc: 0.2500\n",
      "Epoch 12/30\n",
      "33/33 [==============================] - 79s 2s/step - loss: 1.5044 - acc: 0.6793 - val_loss: 5.1974 - val_acc: 0.2125\n",
      "Epoch 13/30\n",
      "33/33 [==============================] - 80s 2s/step - loss: 1.4563 - acc: 0.7006 - val_loss: 5.1272 - val_acc: 0.2500\n",
      "Epoch 14/30\n",
      "33/33 [==============================] - 79s 2s/step - loss: 1.5240 - acc: 0.6915 - val_loss: 5.2593 - val_acc: 0.2375\n",
      "Epoch 15/30\n",
      "33/33 [==============================] - 79s 2s/step - loss: 1.2729 - acc: 0.7582 - val_loss: 5.3073 - val_acc: 0.2375\n",
      "Epoch 16/30\n",
      "33/33 [==============================] - 79s 2s/step - loss: 1.3280 - acc: 0.7643 - val_loss: 5.2749 - val_acc: 0.2500\n",
      "Epoch 17/30\n",
      "33/33 [==============================] - 81s 2s/step - loss: 1.0167 - acc: 0.7909 - val_loss: 5.5233 - val_acc: 0.2000\n",
      "Epoch 18/30\n",
      "33/33 [==============================] - 80s 2s/step - loss: 1.2379 - acc: 0.7521 - val_loss: 5.4457 - val_acc: 0.2375\n",
      "Epoch 19/30\n",
      "33/33 [==============================] - 80s 2s/step - loss: 1.1051 - acc: 0.7552 - val_loss: 5.4863 - val_acc: 0.2625\n",
      "Epoch 20/30\n",
      "33/33 [==============================] - 78s 2s/step - loss: 1.0740 - acc: 0.7831 - val_loss: 5.5561 - val_acc: 0.2625\n",
      "Epoch 21/30\n",
      "33/33 [==============================] - 80s 2s/step - loss: 1.0380 - acc: 0.8037 - val_loss: 5.5305 - val_acc: 0.2125\n",
      "Epoch 22/30\n",
      "33/33 [==============================] - 81s 2s/step - loss: 0.9064 - acc: 0.8424 - val_loss: 5.6512 - val_acc: 0.2500\n",
      "Epoch 23/30\n",
      "33/33 [==============================] - 80s 2s/step - loss: 0.9520 - acc: 0.8098 - val_loss: 5.6424 - val_acc: 0.2375\n",
      "Epoch 24/30\n",
      "33/33 [==============================] - 80s 2s/step - loss: 0.8392 - acc: 0.8280 - val_loss: 5.6316 - val_acc: 0.2125\n",
      "Epoch 25/30\n",
      "33/33 [==============================] - 79s 2s/step - loss: 1.0599 - acc: 0.8037 - val_loss: 5.7483 - val_acc: 0.2250\n",
      "Epoch 26/30\n",
      "33/33 [==============================] - 78s 2s/step - loss: 0.9766 - acc: 0.8014 - val_loss: 5.6567 - val_acc: 0.2750\n",
      "Epoch 27/30\n",
      "33/33 [==============================] - 81s 2s/step - loss: 0.6819 - acc: 0.8667 - val_loss: 5.7493 - val_acc: 0.2250\n",
      "Epoch 28/30\n",
      "33/33 [==============================] - 79s 2s/step - loss: 0.8809 - acc: 0.8613 - val_loss: 5.7510 - val_acc: 0.2625\n",
      "Epoch 29/30\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.7957 - acc: 0.8401 - val_loss: 5.8849 - val_acc: 0.2375\n",
      "Epoch 30/30\n",
      "33/33 [==============================] - 79s 2s/step - loss: 0.7060 - acc: 0.8643 - val_loss: 5.8199 - val_acc: 0.2625\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 10\n",
    "# data augmentation settings\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        color_mode = color_mode,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        color_mode = color_mode,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "test_steps = int(round(train_image_count/batch_size))\n",
    "print('test steps: ' + str(test_steps))\n",
    "validation_steps = int(round(validation_image_count/batch_size))\n",
    "print('validation_steps ' + str(validation_steps))\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=test_steps,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 180, 180, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 89, 89, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 89, 89, 32)   128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 89, 89, 32)   0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 87, 87, 64)   18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 87, 87, 64)   256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 87, 87, 64)   0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 87, 87, 128)  8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 87, 87, 128)  512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 87, 87, 128)  0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 87, 87, 128)  17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 87, 87, 128)  512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 44, 44, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 44, 44, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 44, 44, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 44, 44, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 44, 44, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 44, 44, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 44, 44, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 44, 44, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 44, 44, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 44, 44, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 22, 22, 256)  32768       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 22, 22, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 22, 22, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 22, 22, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 22, 22, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 22, 22, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 22, 22, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 22, 22, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 22, 22, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 22, 22, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 11, 11, 728)  186368      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 11, 11, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 11, 11, 728)  2912        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 11, 11, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 11, 11, 728)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 11, 11, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 11, 11, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 11, 11, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 11, 11, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 11, 11, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 11, 11, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 11, 11, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 11, 11, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 11, 11, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 11, 11, 728)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 11, 11, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 11, 11, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 11, 11, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 11, 11, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 11, 11, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 11, 11, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 11, 11, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 11, 11, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 11, 11, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 11, 11, 728)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 11, 11, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 11, 11, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 11, 11, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 11, 11, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 11, 11, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 11, 11, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 11, 11, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 11, 11, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 11, 11, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 11, 11, 728)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 11, 11, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 11, 11, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 11, 11, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 11, 11, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 11, 11, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 11, 11, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 11, 11, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 11, 11, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 11, 11, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 11, 11, 728)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 11, 11, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 11, 11, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 11, 11, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 11, 11, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 11, 11, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 11, 11, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 11, 11, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 11, 11, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 11, 11, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 11, 11, 728)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 11, 11, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 11, 11, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 11, 11, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 11, 11, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 11, 11, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 11, 11, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 11, 11, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 11, 11, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 11, 11, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 11, 11, 728)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 11, 11, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 11, 11, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 11, 11, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 11, 11, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 11, 11, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 11, 11, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 11, 11, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 11, 11, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 11, 11, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 11, 11, 728)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 11, 11, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 11, 11, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 11, 11, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 11, 11, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 11, 11, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 11, 11, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 11, 11, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 11, 11, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 11, 11, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 11, 11, 728)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 11, 11, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 11, 11, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 11, 11, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 11, 11, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 11, 11, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 6, 6, 1024)   745472      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 6, 6, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 6, 6, 1024)   4096        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 6, 6, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 6, 6, 1536)   1582080     add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 6, 6, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 6, 6, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 6, 6, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 6, 6, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 6, 6, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 20,861,480\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,861,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will unfreeze the layers in block 13 and block 14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == \"block13_sepconv1_act\":\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start fine-tuning our network. We will do this with the RMSprop optimizer, using a very low learning rate. The reason for using a low learning rate is that we want to limit the magnitude of the modifications we make to the representations of the 3 layers that we are fine-tuning. Updates that are too large may harm these representations.\n",
    "\n",
    "Now let's proceed with fine-tuning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test steps: 33\n",
      "validation_steps 8\n",
      "Epoch 1/100\n",
      "33/33 [==============================] - 80s 2s/step - loss: 0.6965 - acc: 0.8522 - val_loss: 5.7882 - val_acc: 0.2625\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 79s 2s/step - loss: 0.6324 - acc: 0.8674 - val_loss: 5.8081 - val_acc: 0.2625\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 79s 2s/step - loss: 0.5944 - acc: 0.8886 - val_loss: 5.8188 - val_acc: 0.2625\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 81s 2s/step - loss: 0.4151 - acc: 0.9242 - val_loss: 5.7868 - val_acc: 0.2375\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 98s 3s/step - loss: 0.7215 - acc: 0.8621 - val_loss: 5.8225 - val_acc: 0.2625\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 136s 4s/step - loss: 0.6875 - acc: 0.8795 - val_loss: 5.7687 - val_acc: 0.2750\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 133s 4s/step - loss: 0.3436 - acc: 0.9455 - val_loss: 5.8618 - val_acc: 0.2750\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 115s 3s/step - loss: 0.6226 - acc: 0.8742 - val_loss: 5.7751 - val_acc: 0.2750\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.4967 - acc: 0.8947 - val_loss: 5.8724 - val_acc: 0.2625\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 86s 3s/step - loss: 0.4187 - acc: 0.9303 - val_loss: 5.9242 - val_acc: 0.2500\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 86s 3s/step - loss: 0.7078 - acc: 0.8765 - val_loss: 5.8248 - val_acc: 0.2875\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 82s 2s/step - loss: 0.6995 - acc: 0.8712 - val_loss: 5.7970 - val_acc: 0.2500\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 93s 3s/step - loss: 0.5077 - acc: 0.9068 - val_loss: 5.7756 - val_acc: 0.2750\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 88s 3s/step - loss: 0.4817 - acc: 0.9098 - val_loss: 5.7922 - val_acc: 0.2750\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 86s 3s/step - loss: 0.4504 - acc: 0.9189 - val_loss: 5.8965 - val_acc: 0.2625\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 81s 2s/step - loss: 0.4826 - acc: 0.9098 - val_loss: 5.8525 - val_acc: 0.2750\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.4239 - acc: 0.9129 - val_loss: 5.9226 - val_acc: 0.2750\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 85s 3s/step - loss: 0.3858 - acc: 0.9333 - val_loss: 5.8760 - val_acc: 0.2875\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.4904 - acc: 0.9311 - val_loss: 5.9214 - val_acc: 0.2750\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 91s 3s/step - loss: 0.4594 - acc: 0.9250 - val_loss: 5.8993 - val_acc: 0.2875\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 93s 3s/step - loss: 0.5635 - acc: 0.8613 - val_loss: 5.9244 - val_acc: 0.2875\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 95s 3s/step - loss: 0.4785 - acc: 0.8947 - val_loss: 5.8987 - val_acc: 0.2625\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 97s 3s/step - loss: 0.5388 - acc: 0.9015 - val_loss: 5.9340 - val_acc: 0.2375\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 92s 3s/step - loss: 0.3598 - acc: 0.9402 - val_loss: 5.9045 - val_acc: 0.2625\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 95s 3s/step - loss: 0.4273 - acc: 0.9129 - val_loss: 5.9115 - val_acc: 0.2750\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.2602 - acc: 0.9636 - val_loss: 5.8951 - val_acc: 0.2875\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.4765 - acc: 0.9341 - val_loss: 5.9332 - val_acc: 0.3000\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 84s 3s/step - loss: 0.4239 - acc: 0.9280 - val_loss: 5.8082 - val_acc: 0.2750\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 81s 2s/step - loss: 0.4312 - acc: 0.9189 - val_loss: 5.9195 - val_acc: 0.2750\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 80s 2s/step - loss: 0.5745 - acc: 0.9068 - val_loss: 5.9593 - val_acc: 0.2875\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 80s 2s/step - loss: 0.4679 - acc: 0.9250 - val_loss: 5.9079 - val_acc: 0.3125\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 82s 2s/step - loss: 0.4293 - acc: 0.9462 - val_loss: 5.8857 - val_acc: 0.2750\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 84s 3s/step - loss: 0.3636 - acc: 0.9432 - val_loss: 5.9033 - val_acc: 0.2875\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 82s 2s/step - loss: 0.3666 - acc: 0.9341 - val_loss: 5.9998 - val_acc: 0.2750\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 92s 3s/step - loss: 0.4254 - acc: 0.9280 - val_loss: 5.9832 - val_acc: 0.2750\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 90s 3s/step - loss: 0.1931 - acc: 0.9697 - val_loss: 5.9555 - val_acc: 0.2750\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 80s 2s/step - loss: 0.5058 - acc: 0.9046 - val_loss: 5.8501 - val_acc: 0.2875\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 114s 3s/step - loss: 0.4272 - acc: 0.9189 - val_loss: 5.9261 - val_acc: 0.2875\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 146s 4s/step - loss: 0.3601 - acc: 0.9220 - val_loss: 5.8749 - val_acc: 0.2875\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 107s 3s/step - loss: 0.4263 - acc: 0.9250 - val_loss: 5.8675 - val_acc: 0.3000\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 113s 3s/step - loss: 0.3401 - acc: 0.9371 - val_loss: 5.8681 - val_acc: 0.2750\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.2422 - acc: 0.9545 - val_loss: 5.9242 - val_acc: 0.2875\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 88s 3s/step - loss: 0.4610 - acc: 0.9046 - val_loss: 5.9492 - val_acc: 0.2625\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 88s 3s/step - loss: 0.3156 - acc: 0.9432 - val_loss: 6.0755 - val_acc: 0.2750\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 82s 2s/step - loss: 0.3258 - acc: 0.9371 - val_loss: 5.9854 - val_acc: 0.2750\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 79s 2s/step - loss: 0.4390 - acc: 0.9432 - val_loss: 6.0195 - val_acc: 0.2750\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 79s 2s/step - loss: 0.4538 - acc: 0.9189 - val_loss: 6.0240 - val_acc: 0.2750\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 80s 2s/step - loss: 0.3700 - acc: 0.9341 - val_loss: 5.8649 - val_acc: 0.2750\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 86s 3s/step - loss: 0.2426 - acc: 0.9553 - val_loss: 5.9775 - val_acc: 0.2500\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 87s 3s/step - loss: 0.1853 - acc: 0.9636 - val_loss: 5.8938 - val_acc: 0.2750\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 82s 2s/step - loss: 0.6116 - acc: 0.8985 - val_loss: 5.9917 - val_acc: 0.2750\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 90s 3s/step - loss: 0.1508 - acc: 0.9758 - val_loss: 5.9705 - val_acc: 0.2875\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.5109 - acc: 0.9137 - val_loss: 5.9751 - val_acc: 0.2750\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 87s 3s/step - loss: 0.1558 - acc: 0.9788 - val_loss: 5.9377 - val_acc: 0.2750\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 93s 3s/step - loss: 0.3490 - acc: 0.9462 - val_loss: 5.9867 - val_acc: 0.2750\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 85s 3s/step - loss: 0.2625 - acc: 0.9432 - val_loss: 6.0747 - val_acc: 0.2875\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 84s 3s/step - loss: 0.3007 - acc: 0.9432 - val_loss: 6.0492 - val_acc: 0.2750\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.2764 - acc: 0.9341 - val_loss: 6.0284 - val_acc: 0.3125\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.3088 - acc: 0.9402 - val_loss: 6.0660 - val_acc: 0.2750\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.3664 - acc: 0.9311 - val_loss: 6.0428 - val_acc: 0.3125\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 85s 3s/step - loss: 0.3399 - acc: 0.9402 - val_loss: 5.9861 - val_acc: 0.2875\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 81s 2s/step - loss: 0.4445 - acc: 0.9167 - val_loss: 6.0091 - val_acc: 0.3000\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 87s 3s/step - loss: 0.1792 - acc: 0.9606 - val_loss: 6.0053 - val_acc: 0.3000\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 91s 3s/step - loss: 0.2641 - acc: 0.9493 - val_loss: 5.9672 - val_acc: 0.2625\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 84s 3s/step - loss: 0.2459 - acc: 0.9493 - val_loss: 6.0987 - val_acc: 0.2500\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 92s 3s/step - loss: 0.2934 - acc: 0.9341 - val_loss: 6.0948 - val_acc: 0.2750\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 101s 3s/step - loss: 0.2373 - acc: 0.9848 - val_loss: 6.0757 - val_acc: 0.2750\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 104s 3s/step - loss: 0.2590 - acc: 0.9341 - val_loss: 6.1544 - val_acc: 0.2625\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 105s 3s/step - loss: 0.2956 - acc: 0.9523 - val_loss: 6.0854 - val_acc: 0.2625\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 98s 3s/step - loss: 0.2815 - acc: 0.9493 - val_loss: 6.0725 - val_acc: 0.3000\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 108s 3s/step - loss: 0.2822 - acc: 0.9493 - val_loss: 6.1211 - val_acc: 0.2750\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 90s 3s/step - loss: 0.2501 - acc: 0.9402 - val_loss: 6.0479 - val_acc: 0.2750\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 89s 3s/step - loss: 0.3017 - acc: 0.9402 - val_loss: 5.9988 - val_acc: 0.3000\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 89s 3s/step - loss: 0.2287 - acc: 0.9462 - val_loss: 5.9904 - val_acc: 0.2875\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 90s 3s/step - loss: 0.2498 - acc: 0.9493 - val_loss: 5.9671 - val_acc: 0.2750\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 89s 3s/step - loss: 0.3139 - acc: 0.9462 - val_loss: 6.0398 - val_acc: 0.2875\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 92s 3s/step - loss: 0.2556 - acc: 0.9553 - val_loss: 6.0129 - val_acc: 0.2750\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 90s 3s/step - loss: 0.2608 - acc: 0.9402 - val_loss: 6.0493 - val_acc: 0.2875\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 91s 3s/step - loss: 0.2944 - acc: 0.9341 - val_loss: 6.0784 - val_acc: 0.2750\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 91s 3s/step - loss: 0.1077 - acc: 0.9727 - val_loss: 6.0788 - val_acc: 0.2750\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 93s 3s/step - loss: 0.4325 - acc: 0.9197 - val_loss: 6.0857 - val_acc: 0.3000\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 92s 3s/step - loss: 0.2850 - acc: 0.9493 - val_loss: 6.0648 - val_acc: 0.2875\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 97s 3s/step - loss: 0.2933 - acc: 0.9432 - val_loss: 6.0577 - val_acc: 0.3125\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 92s 3s/step - loss: 0.2446 - acc: 0.9402 - val_loss: 6.0897 - val_acc: 0.3000\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 94s 3s/step - loss: 0.2282 - acc: 0.9402 - val_loss: 6.0684 - val_acc: 0.2875\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 95s 3s/step - loss: 0.1126 - acc: 0.9758 - val_loss: 6.0821 - val_acc: 0.3000\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 94s 3s/step - loss: 0.2313 - acc: 0.9432 - val_loss: 6.1156 - val_acc: 0.2750\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 88s 3s/step - loss: 0.1731 - acc: 0.9553 - val_loss: 6.1583 - val_acc: 0.3000\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 88s 3s/step - loss: 0.2496 - acc: 0.9493 - val_loss: 6.1479 - val_acc: 0.2875\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 88s 3s/step - loss: 0.3825 - acc: 0.9493 - val_loss: 6.1567 - val_acc: 0.2625\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 91s 3s/step - loss: 0.1735 - acc: 0.9553 - val_loss: 6.2328 - val_acc: 0.2625\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 91s 3s/step - loss: 0.1844 - acc: 0.9584 - val_loss: 6.2278 - val_acc: 0.2750\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 96s 3s/step - loss: 0.2427 - acc: 0.9553 - val_loss: 6.1906 - val_acc: 0.3000\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 94s 3s/step - loss: 0.2320 - acc: 0.9523 - val_loss: 6.2737 - val_acc: 0.2875\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 93s 3s/step - loss: 0.2447 - acc: 0.9341 - val_loss: 6.1817 - val_acc: 0.2750\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 92s 3s/step - loss: 0.5295 - acc: 0.9228 - val_loss: 6.1686 - val_acc: 0.2625\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 94s 3s/step - loss: 0.1084 - acc: 0.9788 - val_loss: 6.1997 - val_acc: 0.2875\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 94s 3s/step - loss: 0.1443 - acc: 0.9788 - val_loss: 6.2716 - val_acc: 0.2875\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 94s 3s/step - loss: 0.1693 - acc: 0.9675 - val_loss: 6.2286 - val_acc: 0.2750\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 91s 3s/step - loss: 0.2345 - acc: 0.9432 - val_loss: 6.1620 - val_acc: 0.2875\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "test_steps = int(round(train_image_count/batch_size))\n",
    "print('test steps: ' + str(test_steps))\n",
    "validation_steps = int(round(validation_image_count/batch_size))\n",
    "print('validation_steps ' + str(validation_steps))\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=test_steps,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on test data and computing Kaggle performance metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "home_dir = os.getcwd()\n",
    "test_dir = os.path.join(home_dir, 'test')\n",
    "test_count = len(os.list_dir(test_dir))\n",
    "ids_count = len(whale_ids)\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "## Want a tensor of length (test, # of whale ids)\n",
    "\n",
    "def predict_labels(directory):\n",
    "    labels = np.zeros(shape=(test_count,ids_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        color_mode = color_mode,\n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        print('Generating predictions for image batch: ' + i)\n",
    "        labels_batch = model.predict(inputs_batch)\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= test_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return  labels\n",
    "\n",
    "test_labels = predict_labels(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = os.list_dir(test_dir)\n",
    "\n",
    "# We want to write something of the form: image_name pred_0 pred_2 ... pred_4\n",
    "# Need to write a function that takes a label vector as input, and outputs an ordered list of 5 \n",
    "# most probable whale_ids\n",
    "\n",
    "# Need to get the index of the top 5 values of the label vector, then convert these into whale_ids\n",
    "\n",
    "def get_ids(label_vec):\n",
    "    ids = ''\n",
    "    for i in range(5):\n",
    "        max_value = max(label_vec)\n",
    "        max_index = label_vec.index(max_value)\n",
    "        ids += whale_ids[max_index]\n",
    "        ids += ' '\n",
    "        label_vec[max_index] = -1\n",
    "    return ids\n",
    "    \n",
    "\n",
    "prediction = 'Image,Id \\n'\n",
    "for i in range(test_count):\n",
    "    prediction += test_list[i]\n",
    "    prediction += ','\n",
    "    prediction += get_ids[test_labels[i]]\n",
    "    prediction += '\\n'\n",
    "\n",
    "    \n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
